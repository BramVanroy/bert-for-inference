{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to inference with BERT\n",
    "\n",
    "This notebook gives an example of how [BERT](https://arxiv.org/abs/1810.04805) can be used to extract\n",
    "sentence embeddings while at the same time giving some information about the model. Note that it does not try\n",
    "to be exhaustive. In some places, links are given as suggestions for further reading. Also note that these days,\n",
    "BERT isn't state of the art anymore. However, the methodology used here can be used in other models such as RoBERTa\n",
    "with minimal changes. Be careful, though, because the differences between model APIs, however small, are incredibly\n",
    "important. For instance, the position of the classification token is not the same for all models. Read the paper,\n",
    "the documentation, or - if you're up for it - the source code! The latter might be a challenge at first, but you \n",
    "learn a lot from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer\n",
    "\n",
    "A deep learning model works with tensors. Tensors are (basically) vectors. Vectors are (basically) a bunch of\n",
    "numbers. To get started, then, the input text (string) needs to be converted into some data type (numbers)\n",
    "that the model can use. This is done by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with a pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "During pretraining, the tokenizer has been \"trained\" as well. It has generated a vocabulary that it \"knows\". Each word \n",
    "has been assigned an index (a number) and that number can then be used in the model. To counter the annoying problem of \n",
    "words that the tokenizer doesn't know yet (out-of-vocabulary or OOV), a special technique is used that ensures that the\n",
    "tokenizer has learnt \"subword units\". That should mean that when using the pretrained models, you won't run into OOV\n",
    "problems. When the tokenizer does not recognize a word (it is not in its vocabulary) it will try to split that word up \n",
    "into smaller parts that it does know. The BERT tokenizer uses [WordPiece](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "to split tokens. As an example, you'll see that `granola` is split into `gran` and `##ola` where `##` indicates the\n",
    "start of the substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
    "granola_ids = tokenizer.encode('granola bars')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))\n",
    "# Convert the IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will probably have noticed the so-called \"special tokens\" [CLS] and [SEP]. These tokens are added automatically by \n",
    "the `.encode()` method so we don't have to worry about them. The first one is a classification token which has been \n",
    "pretrained. It is specifically inserted for any sort of classification task. So instead of having to average of all \n",
    "tokens and use that as a sentence representation, it is recommended to just take the output of the [CLS] which then \n",
    "represents the whole sentence. [SEP], on the other hand, is inserted as a separator between multiple instances. We will\n",
    "not use that here, but it used for things like next sentence prediction where it is a separator between the current and \n",
    "the next sentence. It is especially important to remember the [CLS] token as it can play a great role in classification \n",
    "and regression tasks. \n",
    "\n",
    "We almost have the correct data type to get started. As we saw above, the data type of the token IDs is a list of\n",
    "integers. In this notebook we use the `transformers` library in combination with PyTorch, which works with tensors.\n",
    "A tensor is a special type of optimised list which is typically used in deep learning. To convert our token IDs to a\n",
    "tensor, we can simply put the list in a tensor constructor. Here, we use a `LongTensor` which is used for integers.\n",
    "For floating-point numbers, we'd typically use a `FloatTensor` or just `Tensor`. The `.encode()` method of the \n",
    "tokenizer can return a tensor instead of a list by passing the parameter `return_tensors='pt'` but for illustrative\n",
    "purposes, we will do the conversion from a list to a tensor manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the list of IDs to a tensor of IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n",
    "Now that we have preprocessed our input string into a tensor of IDs, we can feed this to the model. Remember that the \n",
    "IDs are the IDs of a token in the tokenizer's vocabulary. The model \"knows\" which words are being processed because it\n",
    "\"knows\" which token belongs to which ID. In BERT, and in most - if not all - current transformer language models, the\n",
    "first layer is an embedding layer. Each token ID has a embeddings appointed to it. In BERT, the embeddings are the sum\n",
    "of three types of embeddings: the token embedding, the segment embedding, and the position embedding. The token\n",
    "embedding is a value for the given token, the segment embedding indicates whether the segment is the first or the\n",
    "(optional) second one, and the positional embedding distinguishes the position in the input. Below you find a figure\n",
    "from the BERT paper. (See how playing is split in \"play\" and \"##ing\"?) Note that in our case, where we just use BERT\n",
    "for inference of a single sentence, the segmentation embedding is of no importance. For more information, see\n",
    "[this Medium article](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a).  We'll come back to the model architecture later on.\n",
    "\n",
    "![BERT embeddings visualization](img/bert-embeddings.png)\n",
    "\n",
    "(For a very detailed and visual explanation of the whole BERT model, have a look at the explanations on\n",
    "[Jay Alammar's homepage](http://jalammar.github.io/). In particular the \"Illustrated transformer\" is very interesting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we first need to initialize the model. Just like the tokenizer, the model is pretrained which makes it\n",
    "very easy for us to just use the pretrained language model to get some token or sentence representations out of it.\n",
    "Note how we use the same pretrained model as the tokenizer uses (`bert-base-uncased`). This is the smaller BERT model\n",
    "that has been trained on lower case text. Because the model has been trained on lower case text, it does not know cased\n",
    "text. You may hav enoticed that the tokenizer automatically lowercases the text for us. Whether to use a cased or\n",
    "uncased language model really depends on the task. If you think that casing matters (e.g. for NER), you may want to\n",
    "opt for a cased model, otherwise casing might just add noise.\n",
    "\n",
    "In the example below, an additional argument has been given to the model initialisation. `output_hidden_states` will\n",
    "give us more output information. By default, a `BertModel` will return a tuple but the contents of that tuple differ\n",
    "depending on the configuration of the model. When passing `output_hidden_states=True`, the tuple will contain\n",
    "(in order; shape in brackets):\n",
    "\n",
    "1. the last hidden state `(batch_size, sequence_length, hidden_size)`\n",
    "2. the pooler_output of the classification token `(batch_size, hidden_size)`\n",
    "3. the hidden_states of the outputs of the model at each layer and the initial embedding outputs\n",
    "   `(batch_size, sequence_length, hidden_size)`\n",
    "\n",
    "Graphic cards (GPUs) are much better at doing operations on tensors than a CPU is. Therefore, we wish to run our \n",
    "computations on the GPU if it is available. Note that you need to have a GPU available as well as CUDA, and a\n",
    "GPU-accelerated torch version. To increase the calculation speed, we have to move our model to the correct device:\n",
    "if it's available we'll move the model `.to()` the GPU, otherwise it'll stay on the CPU. It is important to remember \n",
    "that the model and the data to process need to be on the same device. This means that we will have to move our \n",
    "`granola_ids` to the same device as the model, too.\n",
    "\n",
    "Finally, we also set the model to evaluation mode (`.eval`) in contrast to training mode (`.train()`). In evluation\n",
    "mode, the model's batchnorm and dropout layers will work in `eval()` mode, e.g. disabling dropout, which you only want\n",
    "during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "granola_ids = granola_ids.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inference\n",
    "The model has been initialized, and the input string has been converted into a tensor. A language model (such as \n",
    "`BertModel` above) has a `forward()` method that is called automatically when calling the object. The forward method \n",
    "basically pushes a given input tensor forward through the model and then returns the output. Since we're only doing\n",
    "inference and not training or fine-tuning the model, this is the only step that involves the model directly to get \n",
    "output. So we don't need to optimize the model (calculate gradients, propagating back). That's quite simple, isn't it?\n",
    "One pecularity is that we set `torch.no_grad()`. This tells the model that we won't be doing any gradient \n",
    "calculation/backpropagation. Ultimately, it makes inference faster and more memory-efficient. You would typically use\n",
    "`model.eval()` (see above) and `torch.no_grad()` together for evaluation and testing of your model. When training the\n",
    "model should be set to `model.train()` and `torch.no_grad()` should *not* be used.\n",
    "\n",
    "In the cell below, you'll see that there's a strange method called `.unsqueeze()`. It \"unsqueezes\" a tensor by adding \n",
    "an extra dimension. In our case, you'll see that our granola tensor of size `(5,)` turns into a different shape of\n",
    "`(1, 5)` where `1` is the dimension of the sentence. These two dimensions are required by the model: it is optimised\n",
    "to train on *batches*. The next paragraph goes into a bit more technical detail but is not required to understand this \n",
    "notebook.\n",
    "\n",
    "A batch consists of multiple input texts at \"the same time\" (typically of the power of two, e.g. 64). With a batch size\n",
    "of 64 (64 sentences at once), the batch size would be `(64, n)` where `64` is the number of sentences, and `n` the\n",
    "sequence length. In this notebook, where we only ever use one input, the following is not important, but if you ever\n",
    "want to fine-tune a model, you'll want to work with batches since the gradient calculation will be better for large\n",
    "batches. In such cases, `n` needs to be the same for all entries; you cannot have one sequence of 5 items and one of\n",
    "12 items. That is where padding comes in - but that is a story for another day. For now, you can remember that the\n",
    "input size of the model needs to be `(n_input_sentences, seq_len)` where `seq_len` can be determined in different ways.\n",
    "Two popular choices are: using the longest text in the batch as `seq_len` (e.g. 12) and padding shorter texts up to\n",
    "this length, or setting a fixed maximal sequence length for the model (typically 512) and pad all items up to this\n",
    "length. The latter approach is easier to implement but is not memory-efficient and is computationally heavier. The\n",
    "choice, as always, is yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(granola_ids.size())\n",
    "# unsqueeze IDs to get batch size of 1 as added dimension\n",
    "granola_ids = granola_ids.unsqueeze(0)\n",
    "print(granola_ids.size())\n",
    "\n",
    "print(type(granola_ids))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=granola_ids)\n",
    "\n",
    "# the output is a tuple\n",
    "print(type(out))\n",
    "# the tuple contains three elements as explained above)\n",
    "print(len(out))\n",
    "# we only want the hidden_states\n",
    "hidden_states = out[2]\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "As discussed above, we push the IDs of our input tokens through the `model()`, which internally calls the model's \n",
    "`forward()` method. `out` is a tuple with all relevant output items (see the list that we discussed earlier on). For us\n",
    "the third item in that tuple is the most important one; it contains all `hidden_states` of the model after a forward\n",
    "pass. `hidden_states` is a tuple of the output of each layer in the model for each token. In the previous\n",
    "cell we saw that the tuple contains 13 items. When you execute the cell below, the architecture of the BertModel is\n",
    "shown (from top-down to the bottom). The `hidden_states` include the output of the `embeddings` layer and the output of\n",
    "all 12 `BertLayer`'s in the encoder. The output of each layer has a size of `(batch_size, sequence_length, 768)`.\n",
    "In our case, that is `(1, 5, 768)` because we only have one input string (batch size of 1), and our input string was\n",
    "tokenized into five IDs (sequence length of 5). `768` is the number of hidden dimensions.\n",
    "\n",
    "The critical reader will notice that there is still one more layer after the encoder, called `pooler`, which is not\n",
    "part of `hidden_states`. This layer is used to \"pool\" the output of the classification token but we will not use that \n",
    "here. Its output is returned in the second item of the output tuple `out`, as discussed before.\n",
    "\n",
    "For an in-depth analysis of BERT's architecture, I'd \n",
    "recommend to read [the paper](https://arxiv.org/abs/1810.04805). However, if you like a more visual explanation, \n",
    "[The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) might be a better place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have all hidden_states, we may want to get a usable value out of it. Let's say that we want to retrieve a\n",
    "sentence embedding by averaging over all tokens. In other words, we want to reduce the size of `(1, 5, 768)` to\n",
    "`(1, 768)` where `1` is the batch size and `768` is the number of hidden dimensions. (One could also call `768` the \n",
    "features that you wish to use in another task.) There are many ways to make a sentence abstraction of tokens, and it \n",
    "often depends on the given task. Here, we will take the mean. For now, we will only use the output of the last layer in\n",
    "the encoder, that is, `hidden_states[-1]`. It is important to indicate that we want to take the `torch.mean`\n",
    "_over a given axis_. Since the size of the output of the layers is `(1, 5, 768)`, we want to average over the five \n",
    "tokens, which are in the second dimension (`dim=1`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**We now have a vector of 768 features representing our input sentence.** But we can do more! The BERT paper discusses\n",
    "how they reached the best results by concatenating the output of the last four layers.\n",
    "\n",
    "![BERT embeddings visualization](img/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "In our example, that means that\n",
    "we need to get the last four layers of `hidden_states` and concatenate them after which we can take the mean. We want\n",
    "to concatenate across the axis of the hidden dimensions of `768`. As a consequence, our concatenated output vector will\n",
    "be of size `(1, 5, 3072)` where `3072=4*768`, i.e. the concatenation of four layers with a hidden dimension of 768. The\n",
    "concatenated vector is much larger than the output of only a single layer, meaning that it contains a lot more features.\n",
    "Do note, as usual, that it depends on your specific task whether these `3072` features perform better than `768`.\n",
    "\n",
    "Having a vector of shape `(1, 5, 3072)`, we still need to take the mean over the token dimension, as we did before. We\n",
    "end up with one feature vector of size `(3072,)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get last four layers\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# cast layers to a tuple and concatenate over the last dimension\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# take the mean of the concatenated vector over the token dimension\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving and loading results\n",
    "\n",
    "It is likely that you want to use your generated feature vector in another model or task and just save them to your \n",
    "hard drive. You can easily save a tensor with `torch.save` and load it in another script with `torch.load`. Typically,\n",
    "the `.pt` (PyTorch) extension is used. Note that you cannot read the saved file with a text editor. It is a pickled\n",
    "object which allows for efficient (de)compression. If you do want to save your tensors in a readable format, you can\n",
    "convert a tensor to numpy and using something like `np.savetxt('tensor.txt', your_tensor.numpy())`. I do not recommend\n",
    "that approach (I'd stick with `torch.save` or another compression technique) but it is possible.\n",
    "\n",
    "See how we use `.cpu()`? `cpu()` tells PyTorch that we want to move the output tensor back from the GPU to the CPU. \n",
    "This is not a required step, but I think it is good practice when doing feature extraction to move your data to CPU so\n",
    "that when you load it, it is also loaded as a CPU tensor rather than a CUDA tensor. Afterwards you can still move \n",
    "things to GPU if need be, but using CPU by default seems like a good idea. Note that a tensor has to be on CPU if you\n",
    "want to convert it to `.numpy()`, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save our created sentence representation\n",
    "torch.save(cat_sentence_embedding.cpu(), 'my_sent_embed.pt')\n",
    "\n",
    "# load it again\n",
    "loaded_tensor = torch.load('my_sent_embed.pt')\n",
    "print(loaded_tensor)\n",
    "print(loaded_tensor.size())\n",
    "\n",
    "# convert it to numpy to use in e.g. sklearn\n",
    "np_loaded_tensor = loaded_tensor.numpy()\n",
    "print(np_loaded_tensor)\n",
    "print(type(np_loaded_tensor))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}