{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to BERT with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A model works with tensors. Tensors are (basically) vectors. Vectors are (basically) numbers. To get started, then, the \n",
    "input text (string) needs to be converted into some data form (numbers) that the model can use. This is done by the \n",
    "tokenizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the tokenizer that is already pretrained\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During pre-training, the tokenizer has been \"trained\" as well. It has generated a vocabulary that it \"knows\". Each word \n",
    "has been assigned an index (a number) and that number can then be used in the model. To counter the annoying problem of \n",
    "words that the tokenizer doesn't know yet (out-of-vocabulary or OOV), a special technique is used that ensures that the\n",
    "tokenizer has learnt \"subword units\". That should mean that when using the pretrained models, you won't run into OOV\n",
    "problems. When the tokenizer does not recognize a word (it is not in its vocabulary) it will try to split that word up \n",
    "into smaller parts that it does know. The BERT tokenizer uses the WordPiece algorithm to split tokens. As an example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "granola_ids [101, 12604, 6030, 6963, 102]\ngranola_tokens ['[CLS]', 'gran', '##ola', 'bars', '[SEP]']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
    "granola_ids = tokenizer.encode('granola bars')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "# Convert the IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will probably have noticed the so-called \"special tokens\" [CLS] and [SEP]. These tokens are added auomatically by \n",
    "the `.encode()` method so we don't have to worry about them. The first one is a classification token which has been \n",
    "pretrained. It is specifically inserted for any sort of classification task. So instead of having to average of all \n",
    "tokens and use that as a sentence representation, it is recommended to just take the output of the [CLS] which then \n",
    "represents the whole sentence. [SEP], on the other hand, is inserted as a separator between multiple instances. We will\n",
    "not use that here, but it used for things like next sentence prediction where it is a separator between the current and \n",
    "the next sentence. It is especially important to remember the [CLS] token as it can play a great role in classification \n",
    "and regression tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}