{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to inference with BERT\n",
    "\n",
    "> This notebook tries to give an example of how BERT can be used to extract contextual embeddings while at the same time\n",
    "giving some information about the model. Note that it does not try to be exhaustive. In some places, links are given as\n",
    "suggestions for further reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The tokenizer\n",
    "\n",
    "A deep learning model works with tensors. Tensors are (basically) vectors. Vectors are (basically) numbers. To get\n",
    "started, then, the input text (string) needs to be converted into some data type (numbers) that the model can use. \n",
    "This is done by the tokenizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with a pretrained model\n",
    "# We'll come back to the `do_lower_case` parameter\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During pre-training, the tokenizer has been \"trained\" as well. It has generated a vocabulary that it \"knows\". Each word \n",
    "has been assigned an index (a number) and that number can then be used in the model. To counter the annoying problem of \n",
    "words that the tokenizer doesn't know yet (out-of-vocabulary or OOV), a special technique is used that ensures that the\n",
    "tokenizer has learnt \"subword units\". That should mean that when using the pretrained models, you won't run into OOV\n",
    "problems. When the tokenizer does not recognize a word (it is not in its vocabulary) it will try to split that word up \n",
    "into smaller parts that it does know. The BERT tokenizer uses the WordPiece algorithm to split tokens. As an example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "granola_ids [101, 12604, 6030, 6963, 102]\ntype of granola_ids <class 'list'>\ngranola_tokens ['[CLS]', 'gran', '##ola', 'bars', '[SEP]']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
    "granola_ids = tokenizer.encode('granola bars')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))\n",
    "# Convert the IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will probably have noticed the so-called \"special tokens\" [CLS] and [SEP]. These tokens are added auomatically by \n",
    "the `.encode()` method so we don't have to worry about them. The first one is a classification token which has been \n",
    "pretrained. It is specifically inserted for any sort of classification task. So instead of having to average of all \n",
    "tokens and use that as a sentence representation, it is recommended to just take the output of the [CLS] which then \n",
    "represents the whole sentence. [SEP], on the other hand, is inserted as a separator between multiple instances. We will\n",
    "not use that here, but it used for things like next sentence prediction where it is a separator between the current and \n",
    "the next sentence. It is especially important to remember the [CLS] token as it can play a great role in classification \n",
    "and regression tasks. \n",
    "\n",
    "We almost have the correct data type to get started. As we saw above, the data type of the token IDs is a list of\n",
    "integers. In this notebook we use the `transformers` library in combination with PyTorch, which works with tensors.\n",
    "A tensor is a special type of optimised list which is typically used in deep learning. To convert our token IDs to a\n",
    "tensor, we can simply put the list in a tensor constructor. Here, we use a `LongTensor` which is used for integers.\n",
    "For floating-point numbers, we'd typically use a `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "granola_ids tensor([  101, 12604,  6030,  6963,   102])\ntype of granola_ids <class 'torch.Tensor'>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Convert the list of IDs to a tensor of IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The model\n",
    "Now that we have preprocessed our input string into a tensor of IDs, we can feed this to the model. Remember that the \n",
    "IDs are the IDs of a token in the tokenizer's vocabulary. The model \"knows\" which words are being processed because it\n",
    "\"knows\" which token belongs to which ID. In BERT, and in most - if not all - current transformer language models, the\n",
    "first layers are embeddings. Each token ID has a embeddings appointed to it. In BERT, the embeddings are the sum of \n",
    "three types of embeddings: the token embedding, the segment embedding, and the position embedding. The token embedding\n",
    "is a value for the given token, the segment embedding indicates whether the segment is the first or the (optional) \n",
    "second one, and the positional embedding distinguishes the position in the input. Below you find a figure from the BERT\n",
    "paper. (See how playing is split in \"play\" and \"##ing\"?) Note that in our case, where we just use BERT for inference\n",
    "of a single sentence, the segmentation embedding is of no importance.\n",
    "For more information, see [this Medium article](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a).\n",
    "\n",
    "![BERT embeddings visualization](img/bert-embeddings.png)\n",
    "\n",
    "(For a very detailed and visual explanation of the whole BERT model, have a look at the explanations on\n",
    "[Jay Alammar's homepage](http://jalammar.github.io/). In particular the \"Illustrated transformer\" is very interesting.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get started, we first need to initialize the model. Just like the tokenizer, the model is pretrained which makes it\n",
    "very easy for us to just use the pretrained language model to get some token or sentence representations out of it.\n",
    "Note how we use the same pretrained model as the tokenizer uses (`bert-base-uncased`). This is the smaller BERT model\n",
    "that has been trained on lower case text. That is also the reason that we passed the argument `do_lower_case`. Because\n",
    "the model has been trained on lower case text, it does not know cased text. If we pass `do_lower_case=True` to the \n",
    "tokenizer, it takes care of casing for us. Whether to use a cased or uncased language model really depends on the task.\n",
    "If you think that casing matters (e.g. for NER), you may want to opt for a cased model.\n",
    "\n",
    "In this example, an additional argument has been given. `output_hidden_states` will give us more output information. \n",
    "By default, a `BertModel` will return a tuple but the contents of that tuple differs depending on the configuration of \n",
    "the model. When passing `output_hidden_states=True`, the tuple will contain (in order; shape in brackets):\n",
    "\n",
    "1. the last hidden state (batch_size, sequence_length, hidden_size)\n",
    "2. the pooler_output of the [CLS] token (batch_size, hidden_size)\n",
    "3. the hidden_states of the outputs of the model at each layer (batch_size, sequence_length, hidden_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model has been initialized, and the input string has been converted into a tensor. A language model (such as \n",
    "`BertModel` above) have a `forward()` method that is called automatically when calling the object. The forward method \n",
    "basically pushes a given input tensor forward through the model and then returns the output."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'tuple'>\n3\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "out = model(input_ids=granola_ids.unsqueeze(0))\n",
    "print(type(out))\n",
    "print(len(out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}